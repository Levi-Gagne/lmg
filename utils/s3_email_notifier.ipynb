{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e209d35-a399-47b9-8a56-0a27c1cf3612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# S3 Email Notifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71081993-9c7c-4b4e-8987-c14e9fe56312",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Script: \n",
    "- S3_Email_Notifier.py\n",
    "\n",
    "This Python script is designed to interact with AWS S3 to verify the presence of specific files,\n",
    "prepare email notifications based on file availability, and generate a YAML file with email contents.\n",
    "This script is intended for use with Apache Spark and utilizes environment variables and command-line\n",
    "arguments to enhance flexibility and ease of configuration for different operational environments.\n",
    "\n",
    "Requirements:\n",
    "- Apache Spark\n",
    "- Python 3\n",
    "- boto3 library\n",
    "- PyYAML library\n",
    "\n",
    "Usage:\n",
    "Execute this script within a Spark session. Customize the script behavior by setting environment\n",
    "variables or by passing command-line arguments.\n",
    "\n",
    "Author:\n",
    "- Levi Gagne\n",
    "\"\"\"\n",
    "\n",
    "# Import required modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "# Constants and Defaults\n",
    "DEFAULT_ENV = 'prd'\n",
    "DEFAULT_BUCKET = 'default_bucket'\n",
    "DEFAULT_ARCHIVE_BUCKET = 'default_archive_bucket'\n",
    "DEFAULT_ARCHIVE_FOLDER = 'archive_folder'\n",
    "DEFAULT_RECIPIENT = 'levi.gagne@outlook.com'\n",
    "\n",
    "# Configuration via Environment Variables\n",
    "env = os.getenv('ENVIRONMENT', DEFAULT_ENV)\n",
    "bucket_name = os.getenv('BUCKET_NAME', DEFAULT_BUCKET)\n",
    "archive_bucket = os.getenv('ARCHIVE_BUCKET', DEFAULT_ARCHIVE_BUCKET)\n",
    "archive_folder = os.getenv('ARCHIVE_FOLDER', DEFAULT_ARCHIVE_FOLDER)\n",
    "email_recipient = os.getenv('EMAIL_RECIPIENT', DEFAULT_RECIPIENT)\n",
    "\n",
    "# ==================================================\n",
    "# Initialize Spark Session\n",
    "# ==================================================\n",
    "spark = SparkSession.builder.appName(\"OnTRAC_STG-INC\").enableHiveSupport().getOrCreate()\n",
    "print(f'Spark Version: {spark.version}')\n",
    "\n",
    "# ==================================================\n",
    "# Setup the AWS S3 Client\n",
    "# ==================================================\n",
    "session = boto3.session.Session()\n",
    "s3_client = session.client(\n",
    "    service_name='s3',\n",
    "    aws_access_key_id=spark.conf.get(\"spark.hadoop.fs.s3a.access.key\"),\n",
    "    aws_secret_access_key=spark.conf.get(\"spark.hadoop.fs.s3a.secret.key\"),\n",
    "    endpoint_url=spark.conf.get(\"spark.hadoop.fs.s3a.endpoint\"),\n",
    "    config=Config(s3={'addressing_style': 'path'})\n",
    ")\n",
    "print(f\"Boto3 Version: {boto3.__version__}\")\n",
    "\n",
    "# Get Current Date in Different Formats\n",
    "current_date = datetime.now()\n",
    "date_format_mmddyyyy = current_date.strftime(\"%m%d%Y\")\n",
    "date_format_yyyymmdd = current_date.strftime(\"%Y%m%d\")\n",
    "file_pattern = f\"OnTRAC_Out_{date_format_mmddyyyy}_new_{date_format_yyyymmdd}.*\\\\.txt\"\n",
    "\n",
    "# Search for Specific Files in S3 Bucket\n",
    "objects = s3_client.list_objects_v2(Bucket=bucket_name)['Contents']\n",
    "matching_files = [obj['Key'] for obj in objects if re.match(file_pattern, obj['Key'])]\n",
    "\n",
    "# Email Content Preparation Function\n",
    "def prepare_email_contents(subject, body, recipient, attachments=None):\n",
    "    \"\"\"Prepare email content dictionary, ensuring required fields and formats.\"\"\"\n",
    "    if not subject or not body or not recipient:\n",
    "        raise ValueError(\"Subject, body, and recipient are required.\")\n",
    "    return {\n",
    "        \"subject\": subject,\n",
    "        \"body\": body,\n",
    "        \"recipient\": recipient,\n",
    "        \"attachments\": attachments if attachments else []\n",
    "    }\n",
    "\n",
    "# Directory for Temporary YAML File\n",
    "directory = f\"/data/commonScripts/DDA/dsr/{env}/{env}-base/pyspark\"\n",
    "\n",
    "# Validate and Possibly Create Directory for Storing the YAML File\n",
    "def validate_yml_path(directory):\n",
    "    \"\"\"Ensure directory exists; create if not.\"\"\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Create YAML File with Email Content\n",
    "def create_email_yml(directory, email_dict):\n",
    "    \"\"\"Write email contents to a YAML file and adjust permissions.\"\"\"\n",
    "    file_path = os.path.join(directory, \"email_contents.yml\")\n",
    "    with open(file_path, \"w\") as file:\n",
    "        yaml.dump(email_dict, file, default_flow_style=False)\n",
    "    os.chmod(file_path, 0o777)\n",
    "    print(f\"Email content saved to {file_path}\")\n",
    "\n",
    "# Processing Based on File Matching\n",
    "if not matching_files:\n",
    "    email_subject = \"Ontrac Out Report Production\"\n",
    "    email_body = f\"No files matched '{file_pattern}' in '{bucket_name}' or '{archive_bucket}/{archive_folder}', and no data for today ({date_format_yyyymmdd}) is in target table. Please check the source and consider manual intervention.\"\n",
    "    email_content = prepare_email_contents(email_subject, email_body, email_recipient)\n",
    "    validate_yml_path(directory)\n",
    "    create_email_yml(directory, [email_content])\n",
    "    print(\"Email content prepared and saved due to no matching files found.\")\n",
    "    spark.stop()\n",
    "    sys.exit(0)\n",
    "else:\n",
    "    print(\"Matching files found, no email will be sent.\")\n",
    "\n",
    "# Shutdown Spark session if script completes without sending an email\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "s3_email_notifier",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
